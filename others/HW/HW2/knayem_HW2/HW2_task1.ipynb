{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2: Task1 - PoS-tagging\n",
    "\n",
    "#### Khandokar Md. Nayem (knayem)\n",
    "\n",
    "#### Approach:\n",
    "We build a improved neural network system using the resources from NLTK as a corpus for supervised Neural Network training and development. We use the Brown corpus as dataset and use all the catagories. As label PoS (Parts of Speech), we use `universal` tags. It has in total 12 types of Pos, they are `.`, `ADP`, `ADV`, `PRT`, `X`, `VERB`, `DET`, `NOUN`, `NUM`, `CONJ`, `ADJ`, `PRON`. \n",
    "\n",
    "We use Word2Vec model to get vector representation of a word. We use N-gram model for this. So we make the language model from the whole Brown corpus and get the 100-dim vector represntation of each word. \n",
    "\n",
    "Brown corpus is huge to train to. So we take only the first 5% of the data and randomly assign 80% data to train set and rest to test set. Each word is represented by the Word2Vec vector and lable for each word is an One-hot encoing vector.\n",
    "\n",
    "We use sequencial Keras model since it is easy to use. The definition of this task demands word to word relations both local and long term. That's why we use bi-directional GRU (Gated Recurrent Unit). We experimented with LSTM too, but GRU is faster than LSTM. In the output layer, we use a dense layer with sigmoid activation function because this is a catagorical task. As loss function, we use cross-entropy and AdamOptimizer algorithm to train. We use 0.0 dropout here. \n",
    "\n",
    "In time of training, we use random suffling of samples. Also we use 5% of train data as validation set. It will prevent the model to overfit as recurrent models are tend to overfit. We use batch size of 64 and 100 epoch. Lastly we use test set to get the final accuracy.\n",
    "\n",
    "#### Result:\n",
    "The best accuracy we get is 98.67%. This is significently higher than the papers reported accuracy (~94%).\n",
    "\n",
    "#### Ideas for improvement:\n",
    "We use only 5% data to train of whole Brown coupus. We should use the whole coupus. Since the coupus is huge, memory efficient minibatching have to be used. Also batch normalization can be a good idea to keep the recurrent cells active. \n",
    "\n",
    "Also we only use N-gram model as vectorization scheme. Other linguistic feature like dependency tree, lemma can be useful to improve the accuracy. So in future, we will incorporate these features in the vectorization scheme and investigate those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, LSTM, GRU, Dense\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK PoS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADV', 'PRT', 'CONJ', 'ADP', 'NUM', 'NOUN', 'PRON', 'VERB', 'X',\n",
       "       'DET', '.', 'ADJ'], dtype='<U4')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagg_sent=brown.tagged_sents(tagset='universal')\n",
    "\n",
    "flat_list = [item[1] for sublist in tagg_sent for item in sublist]\n",
    "Classes = np.array(list(set(flat_list)))\n",
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data=[]\n",
    "pos_data=[]\n",
    "for i in tagg_sent:\n",
    "    pos_tag_list=[]\n",
    "    sent_list=[]\n",
    "    \n",
    "    for tup in i:\n",
    "        sent_list.append(tup[0].lower())\n",
    "        pos_tag_list.append(tup[1])\n",
    "        \n",
    "    result_data.append(sent_list)\n",
    "    pos_data.append(pos_tag_list)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14221"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(result_data,min_count=1)\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Train and Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'atlanta's' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b4b3e51233a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClasses\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClasses\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/b659/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/b659/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/b659/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'atlanta's' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Classes = np.array(['greeting','goodbye','request'])\n",
    "Max_RNN = 100\n",
    "\n",
    "train_set={'x':[],'y':[]}\n",
    "dev_set={'x':[],'y':[]}\n",
    "test_set={'x':[],'y':[]}\n",
    "\n",
    "for index in np.arange(len(pos_data)//20):\n",
    "# for index in np.arange(500):\n",
    "    \n",
    "    part,line = pos_data[index], result_data[index]\n",
    "    \n",
    "    for e in np.arange(Max_RNN):\n",
    "        if e < len(line):\n",
    "            w = line[e]\n",
    "            p = part[e]\n",
    "            \n",
    "            vec = np.array([model.wv[w]],ndmin=2) if e==0 else np.append(vec, [model.wv[w]],axis=0)\n",
    "            pos = np.array(np.where(Classes==p),ndmin=2) if e==0 else np.append(pos, np.where(Classes==p))\n",
    "\n",
    "        else:\n",
    "#             print(np.zeros((1,5)).shape,'w: ',np.zeros((1,5)))\n",
    "            vec = np.zeros((1,100)) if e==0 else np.append(vec, np.zeros((1,100)),axis=0)\n",
    "            pos = np.array(np.where(Classes=='X'),ndmin=2) if e==0 else np.append(pos, np.where(Classes=='X'))\n",
    "#             print(vec.shape,'V: ',vec)\n",
    "            \n",
    "    DataType = np.random.choice(2, 1, p=[0.8, 0.2])\n",
    "    if DataType == 0:\n",
    "        if len(train_set['x'])<1:\n",
    "            train_set['x'] = np.array([vec],ndmin=2)\n",
    "            train_set['y'] = np.array(pos,ndmin=2)\n",
    "        else:\n",
    "            train_set['x'] = np.append(train_set['x'], [vec], axis=0)\n",
    "            train_set['y'] = np.append(train_set['y'], [pos], axis=0)\n",
    "            \n",
    "    elif DataType == 1:\n",
    "        if len(test_set['x'])<1:\n",
    "            test_set['x'] = np.array([vec],ndmin=2)\n",
    "            test_set['y'] = np.array([pos],ndmin=2)\n",
    "        else:\n",
    "            test_set['x'] = np.append(test_set['x'], [vec], axis=0)\n",
    "            test_set['y'] = np.append(test_set['y'], [pos], axis=0)\n",
    "            \n",
    "#     print('X-',train_set['x'].shape, train_set['y'].shape)\n",
    "    \n",
    "# print(train_set['x'].shape)\n",
    "# print( to_categorical(train_set['y'], num_classes=Classes.size) )\n",
    "\n",
    "# # Convert labels to categorical one-hot encoding\n",
    "# # one_hot_labels = keras.utils.to_categorical(train_set['y'], num_classes=3)\n",
    "train_set['y'] = to_categorical(train_set['y'], num_classes=Classes.size)\n",
    "# train_set['y'] = train_set['y'].reshape(train_set['y'].shape[0],train_set['y'].shape[1]*train_set['y'].shape[2])\n",
    "\n",
    "test_set['y'] = to_categorical(test_set['y'], num_classes=Classes.size)\n",
    "# test_set['y'] = test_set['y'].reshape(test_set['y'].shape[0],test_set['y'].shape[1]*test_set['y'].shape[2])\n",
    "\n",
    "print('Q-', train_set['x'].shape )\n",
    "print('Q-', train_set['y'].shape )\n",
    "# print('Q-', train_set['y'] )\n",
    "print('Q-', test_set['x'].shape )\n",
    "print('Q-', test_set['y'].shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(LSTM(Max_RNN, input_shape=(Max_RNN,100)))\n",
    "model.add(Bidirectional(GRU(Max_RNN, return_sequences=True), input_shape=(Max_RNN,100)))\n",
    "model.add(Dense(Classes.size, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(train_set['x'], train_set['y'],validation_split=0.05, shuffle=True, nb_epoch=100, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_set['x'], test_set['y'], verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
